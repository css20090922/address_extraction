{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcx3fXdnkwR6"
      },
      "source": [
        "!pip install transformers\r\n",
        "clear_output()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkH4IM-jxYli"
      },
      "source": [
        "import os\r\n",
        "import re\r\n",
        "import json\r\n",
        "import string\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "import csv\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig,TFBertForTokenClassification,create_optimizer\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPvWlY9BxMAy"
      },
      "source": [
        "MAX_LEN = 128\r\n",
        "configuration = BertConfig()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R2pCb9ylrDp"
      },
      "source": [
        "MODEL_NAME = \"bert-base-cased\"\r\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "save_path = \"bert_base_cased/\"\r\n",
        "if not os.path.exists(save_path):\r\n",
        "    os.makedirs(save_path)\r\n",
        "slow_tokenizer.save_pretrained(save_path)\r\n",
        "\r\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_cased/vocab.txt\", lowercase=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTOCnBeknjWd"
      },
      "source": [
        "df = pd.read_csv('train.csv')\r\n",
        "df.set_index('id', inplace=True)\r\n",
        "\r\n",
        "df['POI'] = df['POI/street'].str.split(\"/\",n=2,expand=True)[0]\r\n",
        "df['street'] = df['POI/street'].str.split(\"/\",n=2,expand=True)[1]\r\n",
        "x_train,x_test = train_test_split(df,test_size=0.20,shuffle=False)\r\n",
        "\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg7rMOCSo88P"
      },
      "source": [
        "記得tokenize前要先個別取出來"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooWLmr89yGcM"
      },
      "source": [
        "class create_dataset:\r\n",
        "  def __init__(self, raw_address, address):\r\n",
        "        self.raw_address = raw_address\r\n",
        "        self.POI = POI\r\n",
        "        self.address = address\r\n",
        "        self.skip = False\r\n",
        "  def preprocess(self):\r\n",
        "    raw_address = self.raw_address\r\n",
        "    POI = self.POI\r\n",
        "    address = self.address\r\n",
        "    \r\n",
        "    # Tokenize tokenized_raw_address\r\n",
        "    tokenized_raw_address = tokenizer.encode(raw_address)\r\n",
        "    tokenized_POI = tokenizer.encode(POI)\r\n",
        "    tokenized_address = tokenizer.encode(address)\r\n",
        "\r\n",
        "    input_ids = tokenized_raw_address\r\n",
        "    \r\n",
        "    attention_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "    padding_length = max_len - len(input_ids)\r\n",
        "\r\n",
        "    if padding_length > 0:  # pad\r\n",
        "        input_ids = input_ids + ([0] * padding_length)\r\n",
        "        attention_mask = attention_mask + ([0] * padding_length)\r\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\r\n",
        "    elif padding_length < 0:  # skip\r\n",
        "        self.skip = True\r\n",
        "        return\r\n",
        "    self.input_ids = input_ids\r\n",
        "    self.attention_mask = attention_mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ve2foHwbu8Hg"
      },
      "source": [
        "test"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}