{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "91IurJ0IM8RD",
        "outputId": "1f6d2c2c-4e3a-4677-9ba7-10921a06cbe8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!git clone https://github.com/css20090922/address_extraction.git"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'address_extraction'...\n",
            "remote: Enumerating objects: 15, done.\u001b[K\n",
            "remote: Counting objects: 100% (15/15), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 15 (delta 2), reused 9 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (15/15), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wQh1GVUNCOX",
        "outputId": "d6235cff-6aab-4455-d622-efb75c64c295",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd address_extraction/"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/address_extraction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcx3fXdnkwR6",
        "outputId": "127dad24-31ae-4686-9b85-7c2ebb7c6b90",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.4.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkH4IM-jxYli"
      },
      "source": [
        "import os\r\n",
        "import re\r\n",
        "import json\r\n",
        "import string\r\n",
        "import numpy as np\r\n",
        "import tensorflow as tf\r\n",
        "import pandas as pd\r\n",
        "import csv\r\n",
        "from tensorflow import keras\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tokenizers import BertWordPieceTokenizer\r\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig,TFBertForTokenClassification,create_optimizer\r\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPvWlY9BxMAy"
      },
      "source": [
        "MAX_LEN = 128\r\n",
        "configuration = BertConfig()"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R2pCb9ylrDp"
      },
      "source": [
        "MODEL_NAME = \"bert-base-cased\"\r\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\r\n",
        "save_path = \"bert_base_cased/\"\r\n",
        "if not os.path.exists(save_path):\r\n",
        "    os.makedirs(save_path)\r\n",
        "slow_tokenizer.save_pretrained(save_path)\r\n",
        "\r\n",
        "tokenizer = BertWordPieceTokenizer(\"bert_base_cased/vocab.txt\", lowercase=True)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTOCnBeknjWd",
        "outputId": "1ad430cd-9209-4f0c-eda9-14eadde0c5c1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "df = pd.read_csv('/content/address_extraction/datasets/train.csv', encoding='latin-1')\r\n",
        "df.set_index('id', inplace=True)\r\n",
        "\r\n",
        "df['POI'] = df['POI/street'].str.split(\"/\",n=2,expand=True)[0]\r\n",
        "df['street'] = df['POI/street'].str.split(\"/\",n=2,expand=True)[1]\r\n",
        "# x_train,x_test = train_test_split(df,test_size=0.20,shuffle=False)\r\n",
        "\r\n",
        "df.head()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw_address</th>\n",
              "      <th>POI/street</th>\n",
              "      <th>POI</th>\n",
              "      <th>street</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jl kapuk timur delta sili iii lippo cika 11 a ...</td>\n",
              "      <td>/jl kapuk timur delta sili iii lippo cika</td>\n",
              "      <td></td>\n",
              "      <td>jl kapuk timur delta sili iii lippo cika</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aye, jati sampurna</td>\n",
              "      <td>/</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>setu siung 119 rt 5 1 13880 cipayung</td>\n",
              "      <td>/siung</td>\n",
              "      <td></td>\n",
              "      <td>siung</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>toko dita, kertosono</td>\n",
              "      <td>toko dita/</td>\n",
              "      <td>toko dita</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jl. orde baru</td>\n",
              "      <td>/jl. orde baru</td>\n",
              "      <td></td>\n",
              "      <td>jl. orde baru</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          raw_address  ...                                    street\n",
              "id                                                     ...                                          \n",
              "0   jl kapuk timur delta sili iii lippo cika 11 a ...  ...  jl kapuk timur delta sili iii lippo cika\n",
              "1                                  aye, jati sampurna  ...                                          \n",
              "2                setu siung 119 rt 5 1 13880 cipayung  ...                                     siung\n",
              "3                                toko dita, kertosono  ...                                          \n",
              "4                                       jl. orde baru  ...                             jl. orde baru\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg7rMOCSo88P"
      },
      "source": [
        "記得tokenize前要先個別取出來"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooWLmr89yGcM"
      },
      "source": [
        "class create_dataset:\r\n",
        "  def __init__(self, raw_address, street, POI):\r\n",
        "        self.raw_address = raw_address\r\n",
        "        self.POI = POI\r\n",
        "        self.street = street\r\n",
        "        self.skip = False\r\n",
        "  def preprocess(self):\r\n",
        "    raw_address = self.raw_address\r\n",
        "    POI = self.POI\r\n",
        "    street = self.street\r\n",
        "    \r\n",
        "    # Tokenize tokenized_raw_address\r\n",
        "    tokenized_raw_address = tokenizer.encode(raw_address)\r\n",
        "    tokenized_POI = tokenizer.encode(POI)\r\n",
        "    tokenized_street = tokenizer.encode(street)\r\n",
        "\r\n",
        "    input_ids = tokenized_raw_address\r\n",
        "    \r\n",
        "    attention_mask = [1] * len(input_ids)\r\n",
        "\r\n",
        "    padding_length = max_len - len(input_ids)\r\n",
        "\r\n",
        "    if padding_length > 0:  # pad\r\n",
        "        input_ids = input_ids + ([0] * padding_length)\r\n",
        "        attention_mask = attention_mask + ([0] * padding_length)\r\n",
        "        token_type_ids = token_type_ids + ([0] * padding_length)\r\n",
        "    elif padding_length < 0:  # skip\r\n",
        "        self.skip = True\r\n",
        "        return\r\n",
        "    self.input_ids = input_ids\r\n",
        "    self.attention_mask = attention_mask"
      ],
      "execution_count": 29,
      "outputs": []
    }
  ]
}