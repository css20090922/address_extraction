{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "91IurJ0IM8RD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8af955e4-eb8e-44e0-9ee8-4f3d81a9962c"
      },
      "source": [
        "!git clone https://github.com/css20090922/address_extraction.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'address_extraction' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wQh1GVUNCOX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15c24c21-d9e3-4591-898d-e108f56bd323"
      },
      "source": [
        "%cd address_extraction/"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/address_extraction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pcx3fXdnkwR6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0050174f-eaeb-4c38-bd5b-2d4c111362ef"
      },
      "source": [
        "!pip install transformers -U"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: transformers in /usr/local/lib/python3.7/dist-packages (4.4.2)\n",
            "Requirement already satisfied, skipping upgrade: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied, skipping upgrade: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.2)\n",
            "Requirement already satisfied, skipping upgrade: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.1)\n",
            "Requirement already satisfied, skipping upgrade: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bkH4IM-jxYli"
      },
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import string\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import csv\n",
        "import matplotlib.pyplot as plt\n",
        "from time import time\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tokenizers import BertWordPieceTokenizer\n",
        "from transformers import BertTokenizer, TFBertModel, BertConfig,TFBertForTokenClassification,create_optimizer\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPvWlY9BxMAy"
      },
      "source": [
        "MAX_LEN = 128\n",
        "ADDR_MAX_LEN = 64\n",
        "configuration = BertConfig()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5R2pCb9ylrDp"
      },
      "source": [
        "MODEL_NAME = \"cahya/bert-base-indonesian-522M\"\n",
        "\n",
        "slow_tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "save_path = \"bert-base-indonesian-522M/\"\n",
        "if not os.path.exists(save_path):\n",
        "    os.makedirs(save_path)\n",
        "slow_tokenizer.save_pretrained(save_path)\n",
        "\n",
        "tokenizer = BertTokenizer(\"bert-base-indonesian-522M/vocab.txt\", lowercase=True,pad_token='0',mask_token='1')"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LTOCnBeknjWd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "ebb0ea83-d698-4b59-c89c-e9ec8699d519"
      },
      "source": [
        "df = pd.read_csv('/content/address_extraction/datasets/train.csv', encoding='latin-1')\n",
        "df.set_index('id', inplace=True)\n",
        "\n",
        "df['POI'] = df['POI/street'].str.split(\"/\",n=2,expand=True)[0]\n",
        "df['street'] = df['POI/street'].str.split(\"/\",n=2,expand=True)[1]\n",
        "\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>raw_address</th>\n",
              "      <th>POI/street</th>\n",
              "      <th>POI</th>\n",
              "      <th>street</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>id</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>jl kapuk timur delta sili iii lippo cika 11 a ...</td>\n",
              "      <td>/jl kapuk timur delta sili iii lippo cika</td>\n",
              "      <td></td>\n",
              "      <td>jl kapuk timur delta sili iii lippo cika</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>aye, jati sampurna</td>\n",
              "      <td>/</td>\n",
              "      <td></td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>setu siung 119 rt 5 1 13880 cipayung</td>\n",
              "      <td>/siung</td>\n",
              "      <td></td>\n",
              "      <td>siung</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>toko dita, kertosono</td>\n",
              "      <td>toko dita/</td>\n",
              "      <td>toko dita</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>jl. orde baru</td>\n",
              "      <td>/jl. orde baru</td>\n",
              "      <td></td>\n",
              "      <td>jl. orde baru</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                          raw_address  ...                                    street\n",
              "id                                                     ...                                          \n",
              "0   jl kapuk timur delta sili iii lippo cika 11 a ...  ...  jl kapuk timur delta sili iii lippo cika\n",
              "1                                  aye, jati sampurna  ...                                          \n",
              "2                setu siung 119 rt 5 1 13880 cipayung  ...                                     siung\n",
              "3                                toko dita, kertosono  ...                                          \n",
              "4                                       jl. orde baru  ...                             jl. orde baru\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bg7rMOCSo88P"
      },
      "source": [
        "記得tokenize前要先個別取出來"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooWLmr89yGcM"
      },
      "source": [
        "class preprocessor:\n",
        "  def __init__(self, raw_address, POI, street):\n",
        "    self.raw_address = raw_address\n",
        "    self.attention_mask = np.zeros((1, MAX_LEN))\n",
        "    self.skip = False\n",
        "\n",
        "    self.label = [] # None: 0, street: 1,  POI: 2\n",
        "    self.POI = str(POI).split()\n",
        "    self.street = str(street).split()\n",
        "\n",
        "  def preprocess(self):\n",
        "    raw_address = self.raw_address\n",
        "    POI = self.POI\n",
        "    street = self.street\n",
        "    # if len(POI) == 0 and len(street) == 0:\n",
        "    if False:\n",
        "      self.skip = True\n",
        "    else:\n",
        "      # Tokenize raw_address\n",
        "      try:\n",
        "        tokenized_raw_address = tokenizer(raw_address,padding='max_length',max_length=MAX_LEN,return_tensors=\"np\")\n",
        "        # tokenized_POI = tokenizer(POI,padding='max_length',max_length=ADDR_MAX_LEN,return_tensors=\"np\",return_attention_mask=False)\n",
        "        # tokenized_street = tokenizer(street,padding='max_length',max_length=ADDR_MAX_LEN,return_tensors=\"np\",return_attention_mask=False)\n",
        "        \n",
        "        self.raw_address = tokenized_raw_address[\"input_ids\"]\n",
        "        self.attention_mask = tokenized_raw_address[\"attention_mask\"]\n",
        "        # self.POI = tokenized_POI[\"input_ids\"]\n",
        "        # self.street = tokenized_street[\"input_ids\"]\n",
        "\n",
        "        for word in raw_address.split(' '):\n",
        "          # remove ',' from word\n",
        "          word = word.replace(',', '')\n",
        "          # print(word)\n",
        "          if word in POI:\n",
        "            self.label.append(2)\n",
        "          elif word in street:\n",
        "            self.label.append(1)\n",
        "          else:\n",
        "            self.label.append(0)\n",
        "        self.label = self.label + [0]*(MAX_LEN - len(self.label))\n",
        "        # print(self.label)\n",
        "        \n",
        "      except (TypeError,AssertionError):\n",
        "        self.skip = True\n",
        "        print(\"raw_address:\"+str(raw_address))\n",
        "        \n",
        "def create_datasets(raw_data):\n",
        "    datasets = []\n",
        " \n",
        "    # split col poi and street\n",
        "    for attr in raw_data:\n",
        "      raw_address = attr[0]\n",
        "      POI = attr[2]\n",
        "      street = attr[3]\n",
        "      \n",
        "      squad_eg = preprocessor(raw_address, POI, street)\n",
        "      squad_eg.preprocess()\n",
        "      datasets.append(squad_eg)\n",
        "    return datasets\n",
        "\n",
        "def create_inputs_targets(token_data):\n",
        "    dataset_dict = {\n",
        "        \"input_ids\": [],\n",
        "        \"attention_mask\": [],\n",
        "        \"label\": []\n",
        "    }\n",
        "    for item in token_data:\n",
        "        if item.skip == False:\n",
        "            dataset_dict[\"input_ids\"].append(item.raw_address[0])\n",
        "            dataset_dict[\"attention_mask\"].append(item.attention_mask[0])               \n",
        "            # dataset_dict[\"POI\"].append(item.POI[0])\n",
        "            # dataset_dict[\"street\"].append(item.street[0])\n",
        "            dataset_dict['label'].append(item.label)\n",
        "\n",
        "    x = [\n",
        "        dataset_dict[\"input_ids\"],\n",
        "        dataset_dict[\"attention_mask\"]\n",
        "    ]\n",
        "    y = dataset_dict['label']\n",
        "    return x, y"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NivTg8Y1IdTH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "258733fe-1576-42f1-8fdb-a5399dcc866c"
      },
      "source": [
        "start = time()\n",
        "# token_df = create_datasets(df[:10].to_numpy().tolist())\n",
        "token_df = create_datasets(df.to_numpy().tolist())\n",
        "x_train, y_train = create_inputs_targets(token_df)\n",
        "x_train = np.array(x_train)\n",
        "y_train = np.array(y_train)\n",
        "print(\"cost {}\".format(time()-start))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cost 107.30913662910461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "501dLUxonNJw",
        "outputId": "1d72e8ec-8a4a-4e02-a4fc-3d223dd4cb51"
      },
      "source": [
        "x_train.shape, y_train.shape"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((2, 300000, 128), (300000, 128))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZ4I0tS7cn0x"
      },
      "source": [
        "def create_model():\n",
        "  encoder = TFBertModel.from_pretrained(\"cahya/bert-base-indonesian-522M\")\n",
        "\n",
        "  input_ids = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
        "  attention_mask = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
        "  embedding = encoder(\n",
        "      input_ids,attention_mask=attention_mask\n",
        "  )[0]\n",
        "\n",
        "  # output_POI = layers.Dense(ADDR_MAX_LEN, name=\"output_POI\", use_bias=False)(embedding)\n",
        "  # output_POI = layers.Flatten()(output_POI)\n",
        "\n",
        "  # output_street = layers.Dense(ADDR_MAX_LEN, name=\"output_street\", use_bias=False)(embedding)\n",
        "  # output_street = layers.Flatten()(output_street)\n",
        "\n",
        "  # output_POI = layers.Activation(keras.activations.softmax)(output_POI)\n",
        "  # output_street = layers.Activation(keras.activations.softmax)(output_street)\n",
        "  dense = layers.Dense(3)(embedding)\n",
        "  # output = layers.Flatten()(dense)\n",
        "\n",
        "  model = keras.Model(\n",
        "      inputs=[input_ids,attention_mask],\n",
        "      # outputs=output\n",
        "      outputs=dense\n",
        "  )\n",
        "\n",
        "  loss = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "  optimizer = keras.optimizers.Adam(lr=5e-5)\n",
        "  model.compile(optimizer=optimizer, loss=[loss, loss])\n",
        "  return model"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "42vjmwnVjZ2E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d77392a-e110-4ccb-d654-807ae91968ad"
      },
      "source": [
        "model = create_model()\n",
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some layers from the model checkpoint at cahya/bert-base-indonesian-522M were not used when initializing TFBertModel: ['mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at cahya/bert-base-indonesian-522M.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "Model: \"model_2\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_5 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_6 (InputLayer)            [(None, 128)]        0                                            \n",
            "__________________________________________________________________________________________________\n",
            "tf_bert_model_2 (TFBertModel)   TFBaseModelOutputWit 110617344   input_5[0][0]                    \n",
            "                                                                 input_6[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 128, 3)       2307        tf_bert_model_2[0][0]            \n",
            "==================================================================================================\n",
            "Total params: 110,619,651\n",
            "Trainable params: 110,619,651\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRtPqmE3ktOl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e65e4a8b-51ee-4eef-fd34-3c241a7e1569"
      },
      "source": [
        "start=time()\n",
        "history = model.fit(\n",
        "    [x_train[0], x_train[1]],\n",
        "    y_train,\n",
        "    epochs=3,  # For demonstration, 3 epochs are recommended\n",
        "    verbose=1,\n",
        "    batch_size=64,\n",
        "    # steps_per_epoch = 100,\n",
        "    validation_split=0.2\n",
        ")\n",
        "print(\"cost\"+str(time()-start))\n",
        "print(history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:The parameters `output_attentions`, `output_hidden_states` and `use_cache` cannot be updated when calling a model.They have to be set to True/False in the config object (i.e.: `config=XConfig.from_pretrained('name', output_attentions=True)`).\n",
            "WARNING:tensorflow:The parameter `return_dict` cannot be set in graph mode and will always be set to `True`.\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_2/bert/pooler/dense/kernel:0', 'tf_bert_model_2/bert/pooler/dense/bias:0'] when minimizing the loss.\n",
            " 397/3750 [==>...........................] - ETA: 1:21:45 - loss: 0.3317"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOyaEnVAvCy2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 236
        },
        "outputId": "74a0831d-b711-42e9-d40f-da7e6837a879"
      },
      "source": [
        "plt.plot(trainacc)\n",
        "plt.plot(valacc)\n",
        "plt.title('Model accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('Model loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Train', 'Val'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-a67f699c6ca9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainacc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalacc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Accuracy'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'trainacc' is not defined"
          ]
        }
      ]
    }
  ]
}